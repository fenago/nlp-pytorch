{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = \"\"\"How that personage haunted my dreams, I need scarcely tell you. On\n",
    "stormy nights, when the wind shook the four corners of the house and\n",
    "the surf roared along the cove and up the cliffs, I would see him in a\n",
    "thousand forms, and with a thousand diabolical expressions. Now the leg\n",
    "would be cut off at the knee, now at the hip, now he was a monstrous\n",
    "kind of a creature who had never had but the one leg, and that in the\n",
    "middle of his body. To see him leap and run and pursue me over hedge and\n",
    "ditch was the worst of nightmares. And altogether I paid pretty dear for\n",
    "my monthly fourpenny piece, in the shape of these abominable fancies\"\"\"\n",
    "\n",
    "text = text.replace(',','').replace('.','').lower().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['haunted', 'my', 'i', 'need'], 'dreams')\n"
     ]
    }
   ],
   "source": [
    "corpus = set(text)\n",
    "corpus_length = len(corpus)\n",
    "\n",
    "word_dict = {}\n",
    "inverse_word_dict = {}\n",
    "\n",
    "for i, word in enumerate(corpus):\n",
    "    word_dict[word] = i\n",
    "    inverse_word_dict[i] = word\n",
    "\n",
    "data = []\n",
    "\n",
    "for i in range(2, len(text) - 2):\n",
    "    sentence = [text[i-2], text[i-1],\n",
    "               text[i+1], text[i+2]]\n",
    "    target = text[i]\n",
    "    data.append((sentence, target))\n",
    "    \n",
    "print(data[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_length = 20\n",
    "\n",
    "class CBoW(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, corpus_length, embedding_dim):\n",
    "        super(CBoW, self).__init__()\n",
    "        \n",
    "        self.embeddings = nn.Embedding(corpus_length, embedding_dim)\n",
    "\n",
    "        self.linear1 = nn.Linear(embedding_dim, 64)\n",
    "        self.linear2 = nn.Linear(64, corpus_length)\n",
    "        \n",
    "        self.activation_function1 = nn.ReLU()\n",
    "        self.activation_function2 = nn.LogSoftmax(dim = -1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = sum(self.embeddings(inputs)).view(1,-1)\n",
    "        out = self.linear1(embeds)\n",
    "        out = self.activation_function1(out)\n",
    "        out = self.linear2(out)\n",
    "        out = self.activation_function2(out)\n",
    "        return out\n",
    "\n",
    "    def get_word_emdedding(self, word):\n",
    "        word = torch.LongTensor([word_dict[word]])\n",
    "        return self.embeddings(word).view(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 9, 48, 43, 70])\n"
     ]
    }
   ],
   "source": [
    "model = CBoW(corpus_length, embedding_length)\n",
    "\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "def make_sentence_vector(sentence, word_dict):\n",
    "    idxs = [word_dict[w] for w in sentence]\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "print(make_sentence_vector(['stormy','nights','when','the'], word_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 540.2003173828125\n",
      "Epoch: 1, Loss: 479.9447937011719\n",
      "Epoch: 2, Loss: 435.550537109375\n",
      "Epoch: 3, Loss: 396.0237731933594\n",
      "Epoch: 4, Loss: 357.2877502441406\n",
      "Epoch: 5, Loss: 318.30047607421875\n",
      "Epoch: 6, Loss: 279.1165771484375\n",
      "Epoch: 7, Loss: 240.94815063476562\n",
      "Epoch: 8, Loss: 204.82603454589844\n",
      "Epoch: 9, Loss: 171.59759521484375\n",
      "Epoch: 10, Loss: 142.0783233642578\n",
      "Epoch: 11, Loss: 116.167236328125\n",
      "Epoch: 12, Loss: 94.1119384765625\n",
      "Epoch: 13, Loss: 75.69329833984375\n",
      "Epoch: 14, Loss: 60.69060516357422\n",
      "Epoch: 15, Loss: 48.82771682739258\n",
      "Epoch: 16, Loss: 39.65796661376953\n",
      "Epoch: 17, Loss: 32.629825592041016\n",
      "Epoch: 18, Loss: 27.246349334716797\n",
      "Epoch: 19, Loss: 23.11562728881836\n",
      "Epoch: 20, Loss: 19.909191131591797\n",
      "Epoch: 21, Loss: 17.365760803222656\n",
      "Epoch: 22, Loss: 15.346717834472656\n",
      "Epoch: 23, Loss: 13.695038795471191\n",
      "Epoch: 24, Loss: 12.3384370803833\n",
      "Epoch: 25, Loss: 11.209527969360352\n",
      "Epoch: 26, Loss: 10.247206687927246\n",
      "Epoch: 27, Loss: 9.425829887390137\n",
      "Epoch: 28, Loss: 8.721075057983398\n",
      "Epoch: 29, Loss: 8.10329818725586\n",
      "Epoch: 30, Loss: 7.563275337219238\n",
      "Epoch: 31, Loss: 7.085591793060303\n",
      "Epoch: 32, Loss: 6.659829616546631\n",
      "Epoch: 33, Loss: 6.280435085296631\n",
      "Epoch: 34, Loss: 5.938225269317627\n",
      "Epoch: 35, Loss: 5.628503322601318\n",
      "Epoch: 36, Loss: 5.348117351531982\n",
      "Epoch: 37, Loss: 5.092507362365723\n",
      "Epoch: 38, Loss: 4.859646320343018\n",
      "Epoch: 39, Loss: 4.644547462463379\n",
      "Epoch: 40, Loss: 4.446408271789551\n",
      "Epoch: 41, Loss: 4.26409387588501\n",
      "Epoch: 42, Loss: 4.094571590423584\n",
      "Epoch: 43, Loss: 3.9374659061431885\n",
      "Epoch: 44, Loss: 3.791804552078247\n",
      "Epoch: 45, Loss: 3.6549158096313477\n",
      "Epoch: 46, Loss: 3.5273609161376953\n",
      "Epoch: 47, Loss: 3.4080140590667725\n",
      "Epoch: 48, Loss: 3.2954890727996826\n",
      "Epoch: 49, Loss: 3.190167188644409\n",
      "Epoch: 50, Loss: 3.090735912322998\n",
      "Epoch: 51, Loss: 2.9966816902160645\n",
      "Epoch: 52, Loss: 2.908266067504883\n",
      "Epoch: 53, Loss: 2.824206829071045\n",
      "Epoch: 54, Loss: 2.745065689086914\n",
      "Epoch: 55, Loss: 2.669590711593628\n",
      "Epoch: 56, Loss: 2.597630262374878\n",
      "Epoch: 57, Loss: 2.5297250747680664\n",
      "Epoch: 58, Loss: 2.465034246444702\n",
      "Epoch: 59, Loss: 2.4032247066497803\n",
      "Epoch: 60, Loss: 2.3440911769866943\n",
      "Epoch: 61, Loss: 2.2879281044006348\n",
      "Epoch: 62, Loss: 2.233961582183838\n",
      "Epoch: 63, Loss: 2.182486057281494\n",
      "Epoch: 64, Loss: 2.1330134868621826\n",
      "Epoch: 65, Loss: 2.08598256111145\n",
      "Epoch: 66, Loss: 2.0404162406921387\n",
      "Epoch: 67, Loss: 1.9969593286514282\n",
      "Epoch: 68, Loss: 1.9551444053649902\n",
      "Epoch: 69, Loss: 1.9148974418640137\n",
      "Epoch: 70, Loss: 1.876097321510315\n",
      "Epoch: 71, Loss: 1.8390111923217773\n",
      "Epoch: 72, Loss: 1.8030190467834473\n",
      "Epoch: 73, Loss: 1.7684489488601685\n",
      "Epoch: 74, Loss: 1.7349504232406616\n",
      "Epoch: 75, Loss: 1.702731966972351\n",
      "Epoch: 76, Loss: 1.6717398166656494\n",
      "Epoch: 77, Loss: 1.6416716575622559\n",
      "Epoch: 78, Loss: 1.6125398874282837\n",
      "Epoch: 79, Loss: 1.5843281745910645\n",
      "Epoch: 80, Loss: 1.5571229457855225\n",
      "Epoch: 81, Loss: 1.5309240818023682\n",
      "Epoch: 82, Loss: 1.5052895545959473\n",
      "Epoch: 83, Loss: 1.4804853200912476\n",
      "Epoch: 84, Loss: 1.4564367532730103\n",
      "Epoch: 85, Loss: 1.4332159757614136\n",
      "Epoch: 86, Loss: 1.4105651378631592\n",
      "Epoch: 87, Loss: 1.3886449337005615\n",
      "Epoch: 88, Loss: 1.3673646450042725\n",
      "Epoch: 89, Loss: 1.3465784788131714\n",
      "Epoch: 90, Loss: 1.326601266860962\n",
      "Epoch: 91, Loss: 1.30686616897583\n",
      "Epoch: 92, Loss: 1.2878540754318237\n",
      "Epoch: 93, Loss: 1.2694076299667358\n",
      "Epoch: 94, Loss: 1.2512110471725464\n",
      "Epoch: 95, Loss: 1.2336981296539307\n",
      "Epoch: 96, Loss: 1.2166565656661987\n",
      "Epoch: 97, Loss: 1.1999529600143433\n",
      "Epoch: 98, Loss: 1.1837033033370972\n",
      "Epoch: 99, Loss: 1.1677820682525635\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "    epoch_loss = 0\n",
    "    for sentence, target in data:\n",
    "        model.zero_grad()\n",
    "        sentence_vector = make_sentence_vector(sentence, word_dict)  \n",
    "        log_probs = model(sentence_vector)\n",
    "        loss = loss_function(log_probs, torch.tensor([word_dict[target]], dtype=torch.long))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.data\n",
    "    print('Epoch: '+str(epoch)+', Loss: ' + str(epoch_loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preceding Words: ['to', 'see']\n",
      "\n",
      "Predicted Word: him\n",
      "\n",
      "Following Words: ['leap', 'and']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_predicted_result(input, inverse_word_dict):\n",
    "    index = np.argmax(input)\n",
    "    return inverse_word_dict[index]\n",
    "\n",
    "def predict_sentence(sentence):\n",
    "    sentence_split = sentence.replace('.','').lower().split()\n",
    "    sentence_vector = make_sentence_vector(sentence_split, word_dict)\n",
    "    prediction_array = model(sentence_vector).data.numpy()\n",
    "    print('Preceding Words: {}\\n'.format(sentence_split[:2]))\n",
    "    print('Predicted Word: {}\\n'.format(get_predicted_result(prediction_array[0], inverse_word_dict)))\n",
    "    print('Following Words: {}\\n'.format(sentence_split[2:]))\n",
    "\n",
    "predict_sentence('to see leap and')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 8.6251e-01, -3.6810e-01, -1.1397e+00, -4.2561e-02,  3.1203e-01,\n",
      "         -6.3440e-01,  1.0768e+00,  2.7745e-01, -5.6835e-01, -2.0671e+00,\n",
      "          2.3117e-01, -5.5059e-02,  1.8441e+00, -1.7148e-01,  8.4483e-01,\n",
      "         -6.5940e-02,  1.2200e+00,  1.0388e+00, -3.9075e-04,  1.3893e+00]],\n",
      "       grad_fn=<ViewBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(model.get_word_emdedding('leap'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorchNLP",
   "language": "python",
   "name": "pytorchnlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
